{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b121babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./miniforge3/lib/python3.9/site-packages (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./miniforge3/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./miniforge3/lib/python3.9/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in ./miniforge3/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./miniforge3/lib/python3.9/site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./miniforge3/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.19 in ./miniforge3/lib/python3.9/site-packages (from matplotlib) (1.21.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./miniforge3/lib/python3.9/site-packages (from matplotlib) (4.37.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniforge3/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./miniforge3/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./miniforge3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660be54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1000)\n",
    "import ast\n",
    "import os\n",
    "import collections\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LightSource\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de636a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/subhrasishchakraborty'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952e04ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/subhrasishchakraborty/Desktop/lumbar-conformal-main/scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e0c269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conformal_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980bd1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/subhrasishchakraborty/Desktop/lumbar-conformal-main/files/lumbar-test-pred.csv')\n",
    "df = df.replace(-1, np.NaN)\n",
    "# df = pd.read_csv('../files/lumbar-test-pred.csv')\n",
    "\n",
    "# exclude first verterbral level as it contains many missing scores\n",
    "exclude = [\n",
    "    'Unnamed: 0', \n",
    "]\n",
    "df.drop(columns=exclude, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f43bca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T12-L1 Left: 66%\n",
      "L1-L2 Left: 88%\n",
      "L2-L3 Left: 80%\n",
      "L3-L4 Left: 69%\n",
      "L4-L5 Left: 61%\n",
      "L5-S1 Left: 66%\n",
      "\n",
      "T12-L1 right: 66%\n",
      "L1-L2 right: 89%\n",
      "L2-L3 right: 79%\n",
      "L3-L4 right: 69%\n",
      "L4-L5 right: 65%\n",
      "L5-S1 right: 66%\n",
      "\n",
      "T12-L1 center: 67%\n",
      "L1-L2 center: 89%\n",
      "L2-L3 center: 83%\n",
      "L3-L4 center: 75%\n",
      "L4-L5 center: 73%\n",
      "L5-S1 center: 85%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_pred = lambda x: np.argmax(ast.literal_eval(x)) if x == x else x\n",
    "print(f'T12-L1 Left: {(df.true_T12_L1_left == df.score_T12_L1_left.map(get_pred)).sum() / df.true_T12_L1_left.count():.0%}')\n",
    "print(f'L1-L2 Left: {(df.true_L1_L2_left == df.score_L1_L2_left.map(get_pred)).sum() / df.true_L1_L2_left.count():.0%}')\n",
    "print(f'L2-L3 Left: {(df.true_L2_L3_left == df.score_L2_L3_left.map(get_pred)).sum() / df.true_L2_L3_left.count():.0%}')\n",
    "print(f'L3-L4 Left: {(df.true_L3_L4_left == df.score_L3_L4_left.map(get_pred)).sum() / df.true_L3_L4_left.count():.0%}')\n",
    "print(f'L4-L5 Left: {(df.true_L4_L5_left == df.score_L4_L5_left.map(get_pred)).sum() / df.true_L4_L5_left.count():.0%}')\n",
    "print(f'L5-S1 Left: {(df.true_L5_S1_left == df.score_L5_S1_left.map(get_pred)).sum() / df.true_L5_S1_left.count():.0%}')\n",
    "print()\n",
    "print(f'T12-L1 right: {(df.true_T12_L1_right == df.score_T12_L1_right.map(get_pred)).sum() / df.true_T12_L1_right.count():.0%}')\n",
    "print(f'L1-L2 right: {(df.true_L1_L2_right == df.score_L1_L2_right.map(get_pred)).sum() / df.true_L1_L2_right.count():.0%}')\n",
    "print(f'L2-L3 right: {(df.true_L2_L3_right == df.score_L2_L3_right.map(get_pred)).sum() / df.true_L2_L3_right.count():.0%}')\n",
    "print(f'L3-L4 right: {(df.true_L3_L4_right == df.score_L3_L4_right.map(get_pred)).sum() / df.true_L3_L4_right.count():.0%}')\n",
    "print(f'L4-L5 right: {(df.true_L4_L5_right == df.score_L4_L5_right.map(get_pred)).sum() / df.true_L4_L5_right.count():.0%}')\n",
    "print(f'L5-S1 right: {(df.true_L5_S1_right == df.score_L5_S1_right.map(get_pred)).sum() / df.true_L5_S1_right.count():.0%}')\n",
    "print()\n",
    "print(f'T12-L1 center: {(df.true_T12_L1_center == df.score_T12_L1_center.map(get_pred)).sum() / df.true_T12_L1_center.count():.0%}')\n",
    "print(f'L1-L2 center: {(df.true_L1_L2_center == df.score_L1_L2_center.map(get_pred)).sum() / df.true_L1_L2_center.count():.0%}')\n",
    "print(f'L2-L3 center: {(df.true_L2_L3_center == df.score_L2_L3_center.map(get_pred)).sum() / df.true_L2_L3_center.count():.0%}')\n",
    "print(f'L3-L4 center: {(df.true_L3_L4_center == df.score_L3_L4_center.map(get_pred)).sum() / df.true_L3_L4_center.count():.0%}')\n",
    "print(f'L4-L5 center: {(df.true_L4_L5_center == df.score_L4_L5_center.map(get_pred)).sum() / df.true_L4_L5_center.count():.0%}')\n",
    "print(f'L5-S1 center: {(df.true_L5_S1_center == df.score_L5_S1_center.map(get_pred)).sum() / df.true_L5_S1_center.count():.0%}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1565efea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left 0.7178136318735594\n",
      "right 0.7257369990062935\n",
      "center 0.785427807486631\n"
     ]
    }
   ],
   "source": [
    "print('left', ((df.true_T12_L1_left == df.score_T12_L1_left.map(get_pred)).sum() + (df.true_L1_L2_left == df.score_L1_L2_left.map(get_pred)).sum()  + (df.true_L2_L3_left == df.score_L2_L3_left.map(get_pred)).sum() + (df.true_L3_L4_left == df.score_L3_L4_left.map(get_pred)).sum()  + (df.true_L4_L5_left == df.score_L4_L5_left.map(get_pred)).sum() + (df.true_L5_S1_left == df.score_L5_S1_left.map(get_pred)).sum()) / (df.true_T12_L1_left.count() + df.true_L1_L2_left.count() + df.true_L2_L3_left.count() + df.true_L3_L4_left.count() + df.true_L4_L5_left.count() + df.true_L5_S1_left.count()))\n",
    "print('right', ((df.true_T12_L1_right == df.score_T12_L1_right.map(get_pred)).sum() + (df.true_L1_L2_right == df.score_L1_L2_right.map(get_pred)).sum()  + (df.true_L2_L3_right == df.score_L2_L3_right.map(get_pred)).sum() + (df.true_L3_L4_right == df.score_L3_L4_right.map(get_pred)).sum()  + (df.true_L4_L5_right == df.score_L4_L5_right.map(get_pred)).sum() + (df.true_L5_S1_right == df.score_L5_S1_right.map(get_pred)).sum()) / (df.true_T12_L1_right.count() + df.true_L1_L2_right.count() + df.true_L2_L3_right.count() + df.true_L3_L4_right.count() + df.true_L4_L5_right.count() + df.true_L5_S1_right.count()))\n",
    "print('center', ((df.true_T12_L1_center == df.score_T12_L1_center.map(get_pred)).sum() + (df.true_L1_L2_center == df.score_L1_L2_center.map(get_pred)).sum()  + (df.true_L2_L3_center == df.score_L2_L3_center.map(get_pred)).sum() + (df.true_L3_L4_center == df.score_L3_L4_center.map(get_pred)).sum()  + (df.true_L4_L5_center == df.score_L4_L5_center.map(get_pred)).sum() + (df.true_L5_S1_center == df.score_L5_S1_center.map(get_pred)).sum()) / (df.true_T12_L1_center.count() + df.true_L1_L2_center.count() + df.true_L2_L3_center.count() + df.true_L3_L4_center.count() + df.true_L4_L5_center.count() + df.true_L5_S1_center.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf749d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = [x for x in df.columns if 'score_' in x]\n",
    "score_df = df[score_cols].dropna()  # drop patients with missing scores\n",
    "\n",
    "label_cols = [x for x in df.columns if 'true_' in x]\n",
    "label_df = df[label_cols]\n",
    "label_df = label_df[label_df.index.isin(score_df.index)]\n",
    "label_df = label_df[~(label_df == -1).all(1)]  # drop patients with no gradings\n",
    "label_df.rename(\n",
    "    columns={x: x.replace('true_', '').upper() + '_TRUE' for x in label_df.columns}, \n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "score_df = score_df[score_df.index.isin(label_df.index)]\n",
    "\n",
    "score_array = np.array([\n",
    "    ast.literal_eval(disc) for patient in score_df.values for disc in patient\n",
    "])\n",
    "label_array = label_df.values.flatten().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e65bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_map = {\n",
    "    i: [i * label_df.shape[1] + j for j, w in enumerate(v) if w == w] \n",
    "    for i, v in enumerate(label_df.values)\n",
    "}\n",
    "\n",
    "for k, v in patient_map.items():  # sanity check\n",
    "    err_msg = f'{k}= {len(v)}= {(label_df.iloc[k] == label_df.iloc[k]).sum()}'\n",
    "    assert len(v) == (label_df.iloc[k] == label_df.iloc[k]).sum(), err_msg\n",
    "\n",
    "clean_index = sum(patient_map.values(), [])\n",
    "clean_score_array = score_array[clean_index]\n",
    "clean_label_array = label_array[clean_index]\n",
    "\n",
    "clean_patient_map = {k: len(v) for k, v in patient_map.items()}\n",
    "\n",
    "start_index = 0\n",
    "end_index = 0\n",
    "for k, v in clean_patient_map.items():\n",
    "    end_index += v\n",
    "    clean_patient_map[k] = list(range(start_index, end_index))\n",
    "    start_index += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75cd3bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8030526834071886"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(clean_score_array.argmax(1) == clean_label_array).sum() / clean_label_array.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b5596cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(conformal_demo)\n",
    "\n",
    "# Experimental parameters\n",
    "num_trials = 100\n",
    "\n",
    "# Range of  miscoverage rates\n",
    "alphas = np.array([0.01, 0.05, 0.10, 0.15, 0.2])\n",
    "\n",
    "# Percentage of data to use for calibration\n",
    "cal_percent = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09a7393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_conditional_eval(sets, labels, classes=['neg', 'mod', 'mil', 'sev']):\n",
    "    res = collections.defaultdict()\n",
    "    for i, v in enumerate(classes):\n",
    "        class_cond_sets = sets[labels == i]\n",
    "        n = class_cond_sets.shape[0]\n",
    "        res[v] = {\n",
    "            'count': n,\n",
    "            'coverage': (class_cond_sets[np.arange(n), i].sum() / n) if n > 0 else 0,\n",
    "            'size': class_cond_sets.sum(1).mean(),\n",
    "        }\n",
    "    return res\n",
    "\n",
    "def size_conditional_eval(sets, labels, sizes=[1, 2, 3, 4]):\n",
    "    res = collections.defaultdict()\n",
    "    for i, v in enumerate(sizes):\n",
    "        size_cond_sets = sets[sets.sum(1) == v]\n",
    "        n = size_cond_sets.shape[0]\n",
    "        res[v] = {\n",
    "            'count': n,\n",
    "            'coverage': (size_cond_sets[np.arange(n), i].sum() / n) if n > 0 else 0,\n",
    "        }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e6a5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_lac_overall_coverage = collections.defaultdict(list)\n",
    "alpha_cdf_overall_coverage = collections.defaultdict(list)\n",
    "alpha_aps_overall_coverage = collections.defaultdict(list)\n",
    "alpha_lac_overall_size = collections.defaultdict(list)\n",
    "alpha_cdf_overall_size = collections.defaultdict(list)\n",
    "alpha_aps_overall_size = collections.defaultdict(list)\n",
    "\n",
    "alpha_lac_class_cond_count = collections.defaultdict(list)\n",
    "alpha_cdf_class_cond_count = collections.defaultdict(list)\n",
    "alpha_aps_class_cond_count = collections.defaultdict(list)\n",
    "alpha_lac_class_cond_coverage = collections.defaultdict(list)\n",
    "alpha_cdf_class_cond_coverage = collections.defaultdict(list)\n",
    "alpha_aps_class_cond_coverage = collections.defaultdict(list)\n",
    "alpha_lac_class_cond_size = collections.defaultdict(list)\n",
    "alpha_cdf_class_cond_size = collections.defaultdict(list)\n",
    "alpha_aps_class_cond_size = collections.defaultdict(list)\n",
    "alpha_lac_size_cond_count = collections.defaultdict(list)\n",
    "alpha_cdf_size_cond_count = collections.defaultdict(list)\n",
    "alpha_aps_size_cond_count = collections.defaultdict(list)\n",
    "alpha_lac_size_cond_coverage = collections.defaultdict(list)\n",
    "alpha_cdf_size_cond_coverage = collections.defaultdict(list)\n",
    "alpha_aps_size_cond_coverage = collections.defaultdict(list)\n",
    "\n",
    "for alpha in alphas:\n",
    "    lac_overall_coverage = []\n",
    "    cdf_overall_coverage = []\n",
    "    aps_overall_coverage = []\n",
    "    lac_overall_size = []\n",
    "    cdf_overall_size = []\n",
    "    aps_overall_size = []\n",
    "\n",
    "    lac_class_cond_count = collections.defaultdict(list)\n",
    "    cdf_class_cond_count = collections.defaultdict(list)\n",
    "    aps_class_cond_count = collections.defaultdict(list)\n",
    "    lac_class_cond_coverage = collections.defaultdict(list)\n",
    "    cdf_class_cond_coverage = collections.defaultdict(list)\n",
    "    aps_class_cond_coverage = collections.defaultdict(list)\n",
    "    lac_class_cond_size = collections.defaultdict(list)\n",
    "    cdf_class_cond_size = collections.defaultdict(list)\n",
    "    aps_class_cond_size = collections.defaultdict(list)\n",
    "    lac_size_cond_count = collections.defaultdict(list)\n",
    "    cdf_size_cond_count = collections.defaultdict(list)\n",
    "    aps_size_cond_count = collections.defaultdict(list)\n",
    "    lac_size_cond_coverage = collections.defaultdict(list)\n",
    "    cdf_size_cond_coverage = collections.defaultdict(list)\n",
    "    aps_size_cond_coverage = collections.defaultdict(list)\n",
    "    lac_size_cond_size = collections.defaultdict(list)\n",
    "    cdf_size_cond_size = collections.defaultdict(list)\n",
    "    aps_size_cond_size = collections.defaultdict(list)\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        rand_patients = np.random.permutation(list(clean_patient_map.keys()))\n",
    "        n = len(rand_patients)\n",
    "        split = int(n * cal_percent)\n",
    "        cal_patients = rand_patients[:split]\n",
    "        val_patients = rand_patients[split:]\n",
    "        cal_indexes = sum([clean_patient_map[k] for k in cal_patients], [])\n",
    "        val_indexes = sum([clean_patient_map[k] for k in val_patients], [])\n",
    "        cal_scores = clean_score_array[cal_indexes]\n",
    "        cal_labels = clean_label_array[cal_indexes]\n",
    "        val_scores = clean_score_array[val_indexes]\n",
    "        val_labels = clean_label_array[val_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14792c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df = pd.DataFrame(cal_scores,cal_labels)\n",
    "val_df = pd.DataFrame(val_scores,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4fc1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df.to_csv('/Users/subhrasishchakraborty/Desktop/cal_df.csv')\n",
    "val_df.to_csv('/Users/subhrasishchakraborty/Desktop/val_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf63de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
